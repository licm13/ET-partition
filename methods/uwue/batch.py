"""Batch processing toolkit for the uWUE evapotranspiration partition method."""

from __future__ import annotations

import argparse
import logging
import sys
import re
from datetime import datetime
from pathlib import Path
from time import time
from typing import Iterable, Optional

import matplotlib.dates as mdates
import matplotlib.pyplot as plt
import numpy as np
import pandas as pd
import seaborn as sns
import xarray as xr
from matplotlib.dates import DateFormatter

from . import bigleaf, zhou
from .preprocess import build_dataset_modified

# è®¾ç½®ç»˜å›¾æ ·å¼
plt.style.use("seaborn-v0_8")
sns.set_palette("husl")

DEFAULT_PATTERN = re.compile(
    r"^(?:AMF|FLX)_.*_FLUXNET(?:2015)?_FULLSET_\d{4}-\d{4}_\d+-\d+$"
)

class uWUEBatchProcessor:
    """
    uWUE æ‰¹å¤„ç†å™¨ç±»
    
    ä¸»è¦åŠŸèƒ½:
    1. æ‰¹é‡å¤„ç† FLUXNET æ•°æ®æ–‡ä»¶
    2. æ‰§è¡Œ Zhou uWUE åˆ†è§£
    3. ç”Ÿæˆå¯è§†åŒ–ç»“æžœ
    4. å¯¼å‡ºå¤„ç†ç»“æžœ
    """
    
    def __init__(
        self,
        base_path: Path,
        output_path: Path,
        create_plots: bool = True,
        folder_pattern: re.Pattern[str] = DEFAULT_PATTERN,
    ) -> None:
        """
        åˆå§‹åŒ–å¤„ç†å™¨
        
        å‚æ•°:
        - base_path: æ•°æ®æºæ ¹ç›®å½•
        - output_path: è¾“å‡ºç›®å½•
        - create_plots: æ˜¯å¦ç”Ÿæˆå¯è§†åŒ–å›¾è¡¨
        """
        self.base_path = Path(base_path)
        self.output_path = Path(output_path)
        self.create_plots = create_plots
        self.folder_pattern = folder_pattern

        # ç¡®ä¿è¾“å‡ºç›®å½•å­˜åœ¨
        self.output_path.mkdir(parents=True, exist_ok=True)

        # è®¾ç½®æ—¥å¿—
        self._setup_logging()

        # å¤„ç†ç»“æžœç»Ÿè®¡
        self.processing_stats = {
            'total_folders': 0,
            'processed_successfully': 0,
            'failed_processing': 0,
            'processing_times': [],
            'sites_processed': []
        }
    
    def _setup_logging(self):
        """è®¾ç½®æ—¥å¿—è®°å½•"""
        log_file = self.output_path / f'uwue_processing_{datetime.now().strftime("%Y%m%d_%H%M%S")}.log'
        
        logging.basicConfig(
            level=logging.INFO,
            format='%(asctime)s - %(levelname)s - %(message)s',
            handlers=[
                logging.FileHandler(log_file, encoding='utf-8'),
                logging.StreamHandler(sys.stdout)
            ]
        )
        self.logger = logging.getLogger(__name__)
    
    def print_header(self):
        """æ‰“å°ç¨‹åºå¤´éƒ¨ä¿¡æ¯"""
        header = """
â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘                            uWUE æ‰¹å¤„ç†åˆ†æžå·¥å…·                                    â•‘
â•‘                                                                                    â•‘
â•‘  ä½œè€…: LCM                                                                         â•‘
â•‘  æ—¥æœŸ: 2025-07-11                                                                  â•‘
â•‘  ååŠ©: Gemini & Claude AI                                                          â•‘
â•‘                                                                                    â•‘
â•‘  åŠŸèƒ½: æ‰¹é‡å¤„ç† FLUXNET æ•°æ®ï¼Œæ‰§è¡Œ Zhou uWUE åˆ†è§£æ–¹æ³•                             â•‘
â•‘        è®¡ç®—è’¸æ•£å‘ä¸­çš„è’¸è…¾éƒ¨åˆ†ï¼Œå¹¶ç”Ÿæˆå¯è§†åŒ–ç»“æžœ                                   â•‘
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
        """
        print(header)
        self.logger.info("uWUE æ‰¹å¤„ç†ç¨‹åºå¯åŠ¨")
        self.logger.info(f"æ•°æ®æºè·¯å¾„: {self.base_path}")
        self.logger.info(f"è¾“å‡ºè·¯å¾„: {self.output_path}")
    
    def scan_directories(self):
        """æ‰«æå¹¶è¯†åˆ«ç¬¦åˆæ¡ä»¶çš„æ•°æ®æ–‡ä»¶å¤¹"""
        if not self.base_path.exists():
            self.logger.error(f"æ•°æ®æºè·¯å¾„ä¸å­˜åœ¨: {self.base_path}")
            return []
        
        all_entries = list(self.base_path.iterdir())
        valid_folders = []
        
        for folder_path in all_entries:
            if folder_path.is_dir() and self.folder_pattern.match(folder_path.name):
                valid_folders.append(folder_path)
        
        self.processing_stats['total_folders'] = len(valid_folders)
        self.logger.info(f"å‘çŽ° {len(valid_folders)} ä¸ªç¬¦åˆæ¡ä»¶çš„æ–‡ä»¶å¤¹")
        
        return valid_folders
    
    def process_single_site(self, folder_path):
        """
        å¤„ç†å•ä¸ªç«™ç‚¹çš„æ•°æ®
        
        å‚æ•°:
        - folder_path: ç«™ç‚¹æ–‡ä»¶å¤¹è·¯å¾„
        
        è¿”å›ž:
        - success: æ˜¯å¦å¤„ç†æˆåŠŸ
        - sitename: ç«™ç‚¹åç§°
        - processing_time: å¤„ç†æ—¶é—´
        - results: å¤„ç†ç»“æžœ (xarray Dataset)
        """
        folder_name = folder_path.name
        start_time = time()
        
        try:
            # 1. æž„å»º CSV æ–‡ä»¶å
            # csv_filename = folder_name.replace('_FLUXNET_FULLSET_', '_FLUXNET_FULLSET_HH_') + '.csv' # AmeriFLUXçš„å‘½åè§„åˆ™
            csv_filename = folder_name.replace('_FLUXNET2015_FULLSET_', '_FLUXNET2015_FULLSET_HH_') + '.csv'  # FLUXNET/æµ‹è¯•é›†çš„å‘½åè§„åˆ™
            csv_filepath = folder_path / csv_filename
            
            if not csv_filepath.exists():
                self.logger.warning(f"CSV æ–‡ä»¶ä¸å­˜åœ¨: {csv_filename}")
                return False, None, 0, None
            
            # 2. æå–ç«™ç‚¹åç§°
            try:
                sitename = csv_filename.split('_')[1]
            except IndexError:
                self.logger.error(f"æ— æ³•ä»Žæ–‡ä»¶åè§£æžç«™ç‚¹åç§°: {csv_filename}")
                return False, None, 0, None
            
            self.logger.info(f"ðŸ”„ å¼€å§‹å¤„ç†ç«™ç‚¹: {sitename}")
            
            # 3. åŠ è½½æ•°æ®
            try:
                ec = build_dataset_modified(str(csv_filepath))
                self.logger.info(f"  âœ… æ•°æ®åŠ è½½å®Œæˆï¼Œæ•°æ®ç‚¹æ•°: {len(ec.time)}")
            except Exception as e:
                self.logger.error(f"  âŒ æ•°æ®åŠ è½½å¤±è´¥: {e}")
                return False, sitename, time() - start_time, None
            
            # 4. æ•°æ®é¢„å¤„ç†å’Œè®¡ç®—
            results = self._perform_uwue_analysis(ec, sitename)
            
            # 5. ä¿å­˜ç»“æžœ
            self._save_results(results, sitename)
            
            # 6. ç”Ÿæˆå¯è§†åŒ–å›¾è¡¨
            if self.create_plots:
                self._create_visualization(results, sitename)
            
            processing_time = time() - start_time
            self.processing_stats['processed_successfully'] += 1
            self.processing_stats['processing_times'].append(processing_time)
            self.processing_stats['sites_processed'].append(sitename)
            
            self.logger.info(f"  âœ… ç«™ç‚¹ {sitename} å¤„ç†å®Œæˆï¼Œè€—æ—¶: {processing_time:.2f} ç§’")
            
            return True, sitename, processing_time, results
            
        except Exception as e:
            self.processing_stats['failed_processing'] += 1
            processing_time = time() - start_time
            self.logger.error(f"  âŒ å¤„ç†ç«™ç‚¹å¤±è´¥: {e}")
            return False, folder_name, processing_time, None
    
    def _perform_uwue_analysis(self, ec, sitename):
        """æ‰§è¡Œ uWUE åˆ†æž"""
        self.logger.info(f"  ðŸ”¬ å¼€å§‹ uWUE åˆ†æž...")
        
        # è®¾ç½®æ—¶é—´æ­¥é•¿å‚æ•° (åŠå°æ—¶æ•°æ®)
        hourlyMask = xr.DataArray(
            np.ones(ec.time.shape).astype(bool), 
            coords=[ec.time], 
            dims=['time']
        )
        nStepsPerDay = 48  # æ¯å¤©48ä¸ªåŠå°æ—¶
        
        # è®¡ç®—è’¸æ•£å‘ (ET)
        ec['ET'] = (bigleaf.LE_to_ET(ec.LE, ec.TA) * 60 * 60 * (24 / nStepsPerDay))
        ec['ET'] = ec['ET'].assign_attrs(
            long_name='evapotranspiration', 
            units='mm per timestep'
        )
        
        # å¡«å……ç¼ºå¤±çš„å‡€è¾å°„æ•°æ®
        missing_netrad = np.isnan(ec['NETRAD'])
        ec['NETRAD'][missing_netrad] = (
            ec['LE'][missing_netrad] + 
            ec['H'][missing_netrad] + 
            ec['G'][missing_netrad]
        )
        
        self.logger.info(f"  ðŸ“Š å¡«å……äº† {missing_netrad.sum().values} ä¸ªç¼ºå¤±çš„å‡€è¾å°„æ•°æ®ç‚¹")
        
        # è®¡ç®—æ½œåœ¨è’¸æ•£å‘ (PET)
        PET, _ = bigleaf.PET(
            ec.TA, ec.PA, ec.NETRAD, 
            G=ec.G, S=None, alpha=1.26,
            missing_G_as_NA=False, 
            missing_S_as_NA=False
        )
        ec['PET'] = PET * 60 * 60 * (24 / nStepsPerDay)
        
        # è®¡ç®— Zhou åˆ†è§£æ‰€éœ€çš„æŽ©ç 
        uWUEa_Mask, uWUEp_Mask = zhou.zhouFlags(
            ec, nStepsPerDay, hourlyMask, GPPvariant='GPP_NT'
        )
        
        self.logger.info(f"  ðŸŽ¯ æœ‰æ•ˆæ•°æ®æŽ©ç : uWUEa={uWUEa_Mask.sum()}, uWUEp={uWUEp_Mask.sum()}")
        
        # å‡†å¤‡æ—¥å‡å€¼æ•°æ®é›†
        ds_zhou = ec[['ET']].resample(time='D').sum(skipna=False)
        ds_zhou['ET'] = ds_zhou['ET'].assign_attrs(
            long_name='evapotranspiration', 
            units='mm d-1'
        )
        
        # åˆå§‹åŒ–è’¸è…¾é‡å˜é‡
        ds_zhou['zhou_T'] = ds_zhou['ET'] * np.nan
        ds_zhou['zhou_T'] = ds_zhou['zhou_T'].assign_attrs(
            long_name='uWUE estimated transpiration (daily uWUEa)',
            units='mm d-1'
        )
        
        ds_zhou['zhou_T_8day'] = ds_zhou['ET'] * np.nan
        ds_zhou['zhou_T_8day'] = ds_zhou['zhou_T_8day'].assign_attrs(
            long_name='uWUE estimated transpiration (8-day moving window)',
            units='mm d-1'
        )
        
        # æŒ‰å¹´ä»½æ‰§è¡Œ Zhou åˆ†è§£
        self.logger.info(f"  ðŸ”„ å¼€å§‹æŒ‰å¹´ä»½æ‰§è¡Œ Zhou åˆ†è§£...")
        ET_vals = ec.ET.values
        GxV_vals = (ec.GPP_NT * np.sqrt(ec.VPD)).values
        
        years = np.unique(ec['time.year'])
        uwue_values = {}
        
        for year in years:
            yearMask = (ec['time.year'] == year).values
            uWUEp, zhou_T, zhou_T_8day = zhou.zhou_part(
                ET_vals[yearMask], GxV_vals[yearMask],
                uWUEa_Mask[yearMask], uWUEp_Mask[yearMask],
                nStepsPerDay, hourlyMask[yearMask],
                rho=95 / 100
            )
            
            ds_zhou['zhou_T'][ds_zhou['time.year'] == year] = zhou_T
            ds_zhou['zhou_T_8day'][ds_zhou['time.year'] == year] = zhou_T_8day
            uwue_values[year] = uWUEp
            
            self.logger.info(f"    - {year} å¹´: uWUEp = {uWUEp:.4f}")
        
        # æ·»åŠ ç«™ç‚¹å’Œå¤„ç†ä¿¡æ¯
        ds_zhou.attrs['sitename'] = sitename
        ds_zhou.attrs['processing_date'] = datetime.now().isoformat()
        ds_zhou.attrs['uwue_values'] = str(uwue_values)
        
        return ds_zhou
    
    def _save_results(self, results, sitename):
        """ä¿å­˜å¤„ç†ç»“æžœ"""
        output_filename = f"{sitename}_uWUE_output.csv"
        output_filepath = self.output_path / output_filename
        
        # ä¿å­˜ä¸º CSV
        results.to_dataframe().to_csv(output_filepath)
        
        # ä¿å­˜ä¸º NetCDF (æ›´é€‚åˆç§‘å­¦æ•°æ®)
        netcdf_filepath = self.output_path / f"{sitename}_uWUE_output.nc"
        results.to_netcdf(netcdf_filepath)
        
        self.logger.info(f"  ðŸ’¾ ç»“æžœå·²ä¿å­˜: {output_filename}")
    
    def _create_visualization(self, results, sitename):
        """åˆ›å»ºå¯è§†åŒ–å›¾è¡¨"""
        try:
            # åˆ›å»ºå›¾è¡¨ç›®å½•
            plots_dir = self.output_path / 'plots'
            plots_dir.mkdir(exist_ok=True)
            
            # è½¬æ¢ä¸º DataFrame ä»¥ä¾¿ç»˜å›¾
            df = results.to_dataframe().reset_index()
            df = df.dropna()  # ç§»é™¤ NaN å€¼
            
            if len(df) == 0:
                self.logger.warning(f"  âš ï¸ ç«™ç‚¹ {sitename} æ— æœ‰æ•ˆæ•°æ®ï¼Œè·³è¿‡ç»˜å›¾")
                return
            
            # åˆ›å»ºå¤šå­å›¾
            fig, axes = plt.subplots(2, 2, figsize=(15, 10))
            fig.suptitle(f'uWUE åˆ†æžç»“æžœ - {sitename}', fontsize=16, fontweight='bold')
            
            # 1. è’¸æ•£å‘æ—¶é—´åºåˆ—
            ax1 = axes[0, 0]
            ax1.plot(df['time'], df['ET'], 'b-', alpha=0.7, label='æ€»è’¸æ•£å‘ (ET)')
            ax1.plot(df['time'], df['zhou_T'], 'r-', alpha=0.8, label='è’¸è…¾ (T)')
            ax1.set_title('è’¸æ•£å‘ä¸Žè’¸è…¾æ—¶é—´åºåˆ—')
            ax1.set_ylabel('è’¸æ•£å‘ (mm dâ»Â¹)')
            ax1.legend()
            ax1.grid(True, alpha=0.3)
            
            # 2. è’¸è…¾æ¯”ä¾‹
            ax2 = axes[0, 1]
            df['T_ET_ratio'] = df['zhou_T'] / df['ET']
            ax2.plot(df['time'], df['T_ET_ratio'], 'g-', alpha=0.7)
            ax2.set_title('è’¸è…¾æ¯”ä¾‹ (T/ET)')
            ax2.set_ylabel('è’¸è…¾æ¯”ä¾‹')
            ax2.set_ylim(0, 1)
            ax2.grid(True, alpha=0.3)
            
            # 3. æ•£ç‚¹å›¾: ET vs T
            ax3 = axes[1, 0]
            scatter = ax3.scatter(df['ET'], df['zhou_T'], c=df.index, cmap='viridis', alpha=0.6)
            ax3.plot([0, df['ET'].max()], [0, df['ET'].max()], 'k--', alpha=0.5, label='1:1 çº¿')
            ax3.set_xlabel('æ€»è’¸æ•£å‘ (mm dâ»Â¹)')
            ax3.set_ylabel('è’¸è…¾ (mm dâ»Â¹)')
            ax3.set_title('è’¸æ•£å‘ vs è’¸è…¾')
            ax3.legend()
            ax3.grid(True, alpha=0.3)
            
            # 4. æœˆå¹³å‡å€¼
            ax4 = axes[1, 1]
            df['month'] = pd.to_datetime(df['time']).dt.month
            monthly_mean = df.groupby('month')[['ET', 'zhou_T']].mean()
            
            months = monthly_mean.index
            ax4.plot(months, monthly_mean['ET'], 'b-o', label='ET')
            ax4.plot(months, monthly_mean['zhou_T'], 'r-o', label='è’¸è…¾')
            ax4.set_title('æœˆå¹³å‡è’¸æ•£å‘')
            ax4.set_xlabel('æœˆä»½')
            ax4.set_ylabel('è’¸æ•£å‘ (mm dâ»Â¹)')
            ax4.set_xticks(range(1, 13))
            ax4.legend()
            ax4.grid(True, alpha=0.3)
            
            # æ ¼å¼åŒ–æ—¶é—´è½´
            for ax in [ax1, ax2]:
                ax.xaxis.set_major_formatter(DateFormatter('%Y'))
                ax.xaxis.set_major_locator(mdates.YearLocator())
            
            plt.tight_layout()
            
            # ä¿å­˜å›¾è¡¨
            plot_filename = plots_dir / f"{sitename}_uWUE_analysis.png"
            plt.savefig(plot_filename, dpi=300, bbox_inches='tight')
            plt.close()
            
            self.logger.info(f"  ðŸ“ˆ å¯è§†åŒ–å›¾è¡¨å·²ä¿å­˜: {plot_filename.name}")
            
        except Exception as e:
            self.logger.error(f"  âŒ åˆ›å»ºå¯è§†åŒ–å›¾è¡¨å¤±è´¥: {e}")
    
    def generate_summary_report(self):
        """ç”Ÿæˆå¤„ç†æ€»ç»“æŠ¥å‘Š"""
        self.logger.info("ðŸ“‹ ç”Ÿæˆå¤„ç†æ€»ç»“æŠ¥å‘Š...")
        
        # è®¡ç®—ç»Ÿè®¡ä¿¡æ¯
        total_time = sum(self.processing_stats['processing_times'])
        avg_time = np.mean(self.processing_stats['processing_times']) if self.processing_stats['processing_times'] else 0
        
        # åˆ›å»ºæ€»ç»“æŠ¥å‘Š
        report = f"""
â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘                                å¤„ç†æ€»ç»“æŠ¥å‘Š                                        â•‘
â• â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•£
â•‘ å¤„ç†æ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}                            â•‘
â•‘ æ€»æ–‡ä»¶å¤¹æ•°: {self.processing_stats['total_folders']}                               â•‘
â•‘ æˆåŠŸå¤„ç†: {self.processing_stats['processed_successfully']}                        â•‘
â•‘ å¤„ç†å¤±è´¥: {self.processing_stats['failed_processing']}                             â•‘
â•‘ æˆåŠŸçŽ‡: {self.processing_stats['processed_successfully']/max(self.processing_stats['total_folders'],1)*100:.1f}%  â•‘
â•‘ æ€»è€—æ—¶: {total_time:.2f} ç§’                                                        â•‘
â•‘ å¹³å‡è€—æ—¶: {avg_time:.2f} ç§’/ç«™ç‚¹                                                   â•‘
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

æˆåŠŸå¤„ç†çš„ç«™ç‚¹:
{chr(10).join([f"  â€¢ {site}" for site in self.processing_stats['sites_processed']])}
        """
        
        print(report)
        
        # ä¿å­˜æŠ¥å‘Šåˆ°æ–‡ä»¶
        report_file = self.output_path / 'processing_summary.txt'
        with open(report_file, 'w', encoding='utf-8') as f:
            f.write(report)
        
        self.logger.info(f"ðŸ“„ æ€»ç»“æŠ¥å‘Šå·²ä¿å­˜: {report_file}")
    
    def run(self):
        """è¿è¡Œæ‰¹å¤„ç†ç¨‹åº"""
        self.print_header()
        
        # æ‰«æç›®å½•
        valid_folders = self.scan_directories()
        
        if not valid_folders:
            self.logger.warning("æœªæ‰¾åˆ°ç¬¦åˆæ¡ä»¶çš„æ–‡ä»¶å¤¹ï¼Œç¨‹åºç»“æŸ")
            return
        
        # å¤„ç†æ¯ä¸ªç«™ç‚¹
        for i, folder_path in enumerate(valid_folders, 1):
            self.logger.info(f"\n{'='*60}")
            self.logger.info(f"å¤„ç†è¿›åº¦: {i}/{len(valid_folders)} ({i/len(valid_folders)*100:.1f}%)")
            self.logger.info(f"å½“å‰æ–‡ä»¶å¤¹: {folder_path.name}")
            
            success, sitename, proc_time, results = self.process_single_site(folder_path)
            
            if success:
                self.logger.info(f"âœ… æˆåŠŸå¤„ç†ç«™ç‚¹ {sitename}")
            else:
                self.logger.error(f"âŒ å¤„ç†å¤±è´¥: {folder_path.name}")
        
        # ç”Ÿæˆæ€»ç»“æŠ¥å‘Š
        self.generate_summary_report()
        
        self.logger.info("\nðŸŽ‰ æ‰€æœ‰å¤„ç†å®Œæˆ!")


def main(argv: Optional[Iterable[str]] = None) -> None:
    """Command-line interface for the uWUE batch workflow."""

    repo_root = Path(__file__).resolve().parents[2]
    default_base = repo_root / "data" / "test_site"
    default_output = repo_root / "outputs" / "uwue"

    parser = argparse.ArgumentParser(description=__doc__)
    parser.add_argument(
        "--base-path",
        type=Path,
        default=default_base,
        help="Directory containing Fluxnet/AmeriFlux style site folders.",
    )
    parser.add_argument(
        "--output-path",
        type=Path,
        default=default_output,
        help="Directory where uWUE results will be stored.",
    )
    parser.add_argument(
        "--no-plots",
        action="store_true",
        help="Disable generation of diagnostic plots.",
    )
    parser.add_argument(
        "--pattern",
        type=str,
        default=DEFAULT_PATTERN.pattern,
        help="Regular expression used to match site folder names.",
    )

    args = parser.parse_args(args=list(argv) if argv is not None else None)

    processor = uWUEBatchProcessor(
        base_path=args.base_path,
        output_path=args.output_path,
        create_plots=not args.no_plots,
        folder_pattern=re.compile(args.pattern),
    )
    processor.run()


if __name__ == "__main__":  # pragma: no cover - CLI entry point
    main()