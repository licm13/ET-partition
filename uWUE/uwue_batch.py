#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
=============================================================================
uWUE æ‰¹å¤„ç†åˆ†æå·¥å…·
=============================================================================
ä½œè€…: LCM
æ—¥æœŸ: 2025-07-11
ååŠ©: Gemini & Claude AI

æè¿°: æœ¬è„šæœ¬ç”¨äºæ‰¹é‡å¤„ç† FLUXNET æ•°æ®ï¼Œæ‰§è¡Œ Zhou ç­‰äººçš„ uWUE åˆ†è§£æ–¹æ³•ï¼Œ
     è®¡ç®—è’¸æ•£å‘ä¸­çš„è’¸è…¾éƒ¨åˆ†ã€‚æ”¯æŒå¯è§†åŒ–ç»“æœå’Œè¿›åº¦ç›‘æ§ã€‚

æ•°æ®å¤„ç†è¯´æ˜:
- æœ¬ä»£ç å¤„ç† AmeriFlux æ•°æ®æ ¼å¼
- ä½¿ç”¨çš„ JSON é…ç½®æ–‡ä»¶å·²é’ˆå¯¹ AmeriFlux è¿›è¡Œä¿®æ”¹å’Œç®€åŒ–
- åŸå§‹ BerkeleyConversion_original.json ä¸º Jacob Nelson æä¾›ï¼Œä¸“ç”¨äº FLUXNET2015 æ•°æ®
- å½“å‰ JSON é…ç½®é€‚é… AmeriFlux æ•°æ®ç»“æ„ï¼Œå»é™¤äº†ä¸å¿…è¦çš„å¤æ‚æ€§

uWUE æ–¹æ³•å‚è€ƒ:
Zhou, S., et al. (2016). uWUE paper WRR
=============================================================================
"""

import os
import re
import sys
from time import time
from datetime import datetime
import logging
from pathlib import Path

# æ ¸å¿ƒæ•°æ®å¤„ç†åº“
import xarray as xr
import numpy as np
import pandas as pd

# å¯è§†åŒ–åº“
import matplotlib.pyplot as plt
import seaborn as sns
from matplotlib.dates import DateFormatter
import matplotlib.dates as mdates

# è‡ªå®šä¹‰æ¨¡å—
try:
    from preprocess import build_dataset_modified
    import bigleaf
    import zhou
except ImportError as e:
    print(f"âŒ ç¼ºå°‘å¿…è¦çš„æ¨¡å—: {e}")
    print("è¯·ç¡®ä¿å·²æ­£ç¡®å®‰è£…å¹¶å¯¼å…¥ preprocess, bigleaf, zhou æ¨¡å—")
    sys.exit(1)

# è®¾ç½®ç»˜å›¾æ ·å¼
plt.style.use('seaborn-v0_8')
sns.set_palette("husl")

class uWUEBatchProcessor:
    """
    uWUE æ‰¹å¤„ç†å™¨ç±»
    
    ä¸»è¦åŠŸèƒ½:
    1. æ‰¹é‡å¤„ç† FLUXNET æ•°æ®æ–‡ä»¶
    2. æ‰§è¡Œ Zhou uWUE åˆ†è§£
    3. ç”Ÿæˆå¯è§†åŒ–ç»“æœ
    4. å¯¼å‡ºå¤„ç†ç»“æœ
    """
    
    def __init__(self, base_path, output_path, create_plots=True):
        """
        åˆå§‹åŒ–å¤„ç†å™¨
        
        å‚æ•°:
        - base_path: æ•°æ®æºæ ¹ç›®å½•
        - output_path: è¾“å‡ºç›®å½•
        - create_plots: æ˜¯å¦ç”Ÿæˆå¯è§†åŒ–å›¾è¡¨
        """
        self.base_path = Path(base_path)
        self.output_path = Path(output_path)
        self.create_plots = create_plots
        
        # ç¡®ä¿è¾“å‡ºç›®å½•å­˜åœ¨
        self.output_path.mkdir(parents=True, exist_ok=True)
        
        # è®¾ç½®æ—¥å¿—
        self._setup_logging()
        
        # æ–‡ä»¶å¤¹åç§°åŒ¹é…æ¨¡å¼
        # æ³¨é‡Šéƒ¨åˆ†ä¸ºæµ‹è¯•ç«™ç‚¹FLX_FI-Hyy_FLUXNET2015_FULLSET_HH_2008-2010_1-3.csv
        # self.folder_pattern = re.compile(r'^AMF_.*_FLUXNET_FULLSET_\d{4}-\d{4}_\d+-\d+$') # AmeriFLUXçš„å‘½åè§„åˆ™
        self.folder_pattern = re.compile(r'^FLX_.*_FLUXNET2015_FULLSET_\d{4}-\d{4}_\d+-\d+$') # FLUXNETå’Œæµ‹è¯•é›†çš„å‘½åè§„åˆ™

        # å¤„ç†ç»“æœç»Ÿè®¡
        self.processing_stats = {
            'total_folders': 0,
            'processed_successfully': 0,
            'failed_processing': 0,
            'processing_times': [],
            'sites_processed': []
        }
    
    def _setup_logging(self):
        """è®¾ç½®æ—¥å¿—è®°å½•"""
        log_file = self.output_path / f'uwue_processing_{datetime.now().strftime("%Y%m%d_%H%M%S")}.log'
        
        logging.basicConfig(
            level=logging.INFO,
            format='%(asctime)s - %(levelname)s - %(message)s',
            handlers=[
                logging.FileHandler(log_file, encoding='utf-8'),
                logging.StreamHandler(sys.stdout)
            ]
        )
        self.logger = logging.getLogger(__name__)
    
    def print_header(self):
        """æ‰“å°ç¨‹åºå¤´éƒ¨ä¿¡æ¯"""
        header = """
â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘                            uWUE æ‰¹å¤„ç†åˆ†æå·¥å…·                                    â•‘
â•‘                                                                                    â•‘
â•‘  ä½œè€…: LCM                                                                         â•‘
â•‘  æ—¥æœŸ: 2025-07-11                                                                  â•‘
â•‘  ååŠ©: Gemini & Claude AI                                                          â•‘
â•‘                                                                                    â•‘
â•‘  åŠŸèƒ½: æ‰¹é‡å¤„ç† FLUXNET æ•°æ®ï¼Œæ‰§è¡Œ Zhou uWUE åˆ†è§£æ–¹æ³•                             â•‘
â•‘        è®¡ç®—è’¸æ•£å‘ä¸­çš„è’¸è…¾éƒ¨åˆ†ï¼Œå¹¶ç”Ÿæˆå¯è§†åŒ–ç»“æœ                                   â•‘
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
        """
        print(header)
        self.logger.info("uWUE æ‰¹å¤„ç†ç¨‹åºå¯åŠ¨")
        self.logger.info(f"æ•°æ®æºè·¯å¾„: {self.base_path}")
        self.logger.info(f"è¾“å‡ºè·¯å¾„: {self.output_path}")
    
    def scan_directories(self):
        """æ‰«æå¹¶è¯†åˆ«ç¬¦åˆæ¡ä»¶çš„æ•°æ®æ–‡ä»¶å¤¹"""
        if not self.base_path.exists():
            self.logger.error(f"æ•°æ®æºè·¯å¾„ä¸å­˜åœ¨: {self.base_path}")
            return []
        
        all_entries = list(self.base_path.iterdir())
        valid_folders = []
        
        for folder_path in all_entries:
            if folder_path.is_dir() and self.folder_pattern.match(folder_path.name):
                valid_folders.append(folder_path)
        
        self.processing_stats['total_folders'] = len(valid_folders)
        self.logger.info(f"å‘ç° {len(valid_folders)} ä¸ªç¬¦åˆæ¡ä»¶çš„æ–‡ä»¶å¤¹")
        
        return valid_folders
    
    def process_single_site(self, folder_path):
        """
        å¤„ç†å•ä¸ªç«™ç‚¹çš„æ•°æ®
        
        å‚æ•°:
        - folder_path: ç«™ç‚¹æ–‡ä»¶å¤¹è·¯å¾„
        
        è¿”å›:
        - success: æ˜¯å¦å¤„ç†æˆåŠŸ
        - sitename: ç«™ç‚¹åç§°
        - processing_time: å¤„ç†æ—¶é—´
        - results: å¤„ç†ç»“æœ (xarray Dataset)
        """
        folder_name = folder_path.name
        start_time = time()
        
        try:
            # 1. æ„å»º CSV æ–‡ä»¶å
            # csv_filename = folder_name.replace('_FLUXNET_FULLSET_', '_FLUXNET_FULLSET_HH_') + '.csv' # AmeriFLUXçš„å‘½åè§„åˆ™
            csv_filename = folder_name.replace('_FLUXNET2015_FULLSET_', '_FLUXNET2015_FULLSET_HH_') + '.csv'  # FLUXNET/æµ‹è¯•é›†çš„å‘½åè§„åˆ™
            csv_filepath = folder_path / csv_filename
            
            if not csv_filepath.exists():
                self.logger.warning(f"CSV æ–‡ä»¶ä¸å­˜åœ¨: {csv_filename}")
                return False, None, 0, None
            
            # 2. æå–ç«™ç‚¹åç§°
            try:
                sitename = csv_filename.split('_')[1]
            except IndexError:
                self.logger.error(f"æ— æ³•ä»æ–‡ä»¶åè§£æç«™ç‚¹åç§°: {csv_filename}")
                return False, None, 0, None
            
            self.logger.info(f"ğŸ”„ å¼€å§‹å¤„ç†ç«™ç‚¹: {sitename}")
            
            # 3. åŠ è½½æ•°æ®
            try:
                ec = build_dataset_modified(str(csv_filepath))
                self.logger.info(f"  âœ… æ•°æ®åŠ è½½å®Œæˆï¼Œæ•°æ®ç‚¹æ•°: {len(ec.time)}")
            except Exception as e:
                self.logger.error(f"  âŒ æ•°æ®åŠ è½½å¤±è´¥: {e}")
                return False, sitename, time() - start_time, None
            
            # 4. æ•°æ®é¢„å¤„ç†å’Œè®¡ç®—
            results = self._perform_uwue_analysis(ec, sitename)
            
            # 5. ä¿å­˜ç»“æœ
            self._save_results(results, sitename)
            
            # 6. ç”Ÿæˆå¯è§†åŒ–å›¾è¡¨
            if self.create_plots:
                self._create_visualization(results, sitename)
            
            processing_time = time() - start_time
            self.processing_stats['processed_successfully'] += 1
            self.processing_stats['processing_times'].append(processing_time)
            self.processing_stats['sites_processed'].append(sitename)
            
            self.logger.info(f"  âœ… ç«™ç‚¹ {sitename} å¤„ç†å®Œæˆï¼Œè€—æ—¶: {processing_time:.2f} ç§’")
            
            return True, sitename, processing_time, results
            
        except Exception as e:
            self.processing_stats['failed_processing'] += 1
            processing_time = time() - start_time
            self.logger.error(f"  âŒ å¤„ç†ç«™ç‚¹å¤±è´¥: {e}")
            return False, folder_name, processing_time, None
    
    def _perform_uwue_analysis(self, ec, sitename):
        """æ‰§è¡Œ uWUE åˆ†æ"""
        self.logger.info(f"  ğŸ”¬ å¼€å§‹ uWUE åˆ†æ...")
        
        # è®¾ç½®æ—¶é—´æ­¥é•¿å‚æ•° (åŠå°æ—¶æ•°æ®)
        hourlyMask = xr.DataArray(
            np.ones(ec.time.shape).astype(bool), 
            coords=[ec.time], 
            dims=['time']
        )
        nStepsPerDay = 48  # æ¯å¤©48ä¸ªåŠå°æ—¶
        
        # è®¡ç®—è’¸æ•£å‘ (ET)
        ec['ET'] = (bigleaf.LE_to_ET(ec.LE, ec.TA) * 60 * 60 * (24 / nStepsPerDay))
        ec['ET'] = ec['ET'].assign_attrs(
            long_name='evapotranspiration', 
            units='mm per timestep'
        )
        
        # å¡«å……ç¼ºå¤±çš„å‡€è¾å°„æ•°æ®
        missing_netrad = np.isnan(ec['NETRAD'])
        ec['NETRAD'][missing_netrad] = (
            ec['LE'][missing_netrad] + 
            ec['H'][missing_netrad] + 
            ec['G'][missing_netrad]
        )
        
        self.logger.info(f"  ğŸ“Š å¡«å……äº† {missing_netrad.sum().values} ä¸ªç¼ºå¤±çš„å‡€è¾å°„æ•°æ®ç‚¹")
        
        # è®¡ç®—æ½œåœ¨è’¸æ•£å‘ (PET)
        PET, _ = bigleaf.PET(
            ec.TA, ec.PA, ec.NETRAD, 
            G=ec.G, S=None, alpha=1.26,
            missing_G_as_NA=False, 
            missing_S_as_NA=False
        )
        ec['PET'] = PET * 60 * 60 * (24 / nStepsPerDay)
        
        # è®¡ç®— Zhou åˆ†è§£æ‰€éœ€çš„æ©ç 
        uWUEa_Mask, uWUEp_Mask = zhou.zhouFlags(
            ec, nStepsPerDay, hourlyMask, GPPvariant='GPP_NT'
        )
        
        self.logger.info(f"  ğŸ¯ æœ‰æ•ˆæ•°æ®æ©ç : uWUEa={uWUEa_Mask.sum()}, uWUEp={uWUEp_Mask.sum()}")
        
        # å‡†å¤‡æ—¥å‡å€¼æ•°æ®é›†
        ds_zhou = ec[['ET']].resample(time='D').sum(skipna=False)
        ds_zhou['ET'] = ds_zhou['ET'].assign_attrs(
            long_name='evapotranspiration', 
            units='mm d-1'
        )
        
        # åˆå§‹åŒ–è’¸è…¾é‡å˜é‡
        ds_zhou['zhou_T'] = ds_zhou['ET'] * np.nan
        ds_zhou['zhou_T'] = ds_zhou['zhou_T'].assign_attrs(
            long_name='uWUE estimated transpiration (daily uWUEa)',
            units='mm d-1'
        )
        
        ds_zhou['zhou_T_8day'] = ds_zhou['ET'] * np.nan
        ds_zhou['zhou_T_8day'] = ds_zhou['zhou_T_8day'].assign_attrs(
            long_name='uWUE estimated transpiration (8-day moving window)',
            units='mm d-1'
        )
        
        # æŒ‰å¹´ä»½æ‰§è¡Œ Zhou åˆ†è§£
        self.logger.info(f"  ğŸ”„ å¼€å§‹æŒ‰å¹´ä»½æ‰§è¡Œ Zhou åˆ†è§£...")
        ET_vals = ec.ET.values
        GxV_vals = (ec.GPP_NT * np.sqrt(ec.VPD)).values
        
        years = np.unique(ec['time.year'])
        uwue_values = {}
        
        for year in years:
            yearMask = (ec['time.year'] == year).values
            uWUEp, zhou_T, zhou_T_8day = zhou.zhou_part(
                ET_vals[yearMask], GxV_vals[yearMask],
                uWUEa_Mask[yearMask], uWUEp_Mask[yearMask],
                nStepsPerDay, hourlyMask[yearMask],
                rho=95 / 100
            )
            
            ds_zhou['zhou_T'][ds_zhou['time.year'] == year] = zhou_T
            ds_zhou['zhou_T_8day'][ds_zhou['time.year'] == year] = zhou_T_8day
            uwue_values[year] = uWUEp
            
            self.logger.info(f"    - {year} å¹´: uWUEp = {uWUEp:.4f}")
        
        # æ·»åŠ ç«™ç‚¹å’Œå¤„ç†ä¿¡æ¯
        ds_zhou.attrs['sitename'] = sitename
        ds_zhou.attrs['processing_date'] = datetime.now().isoformat()
        ds_zhou.attrs['uwue_values'] = str(uwue_values)
        
        return ds_zhou
    
    def _save_results(self, results, sitename):
        """ä¿å­˜å¤„ç†ç»“æœ"""
        output_filename = f"{sitename}_uWUE_output.csv"
        output_filepath = self.output_path / output_filename
        
        # ä¿å­˜ä¸º CSV
        results.to_dataframe().to_csv(output_filepath)
        
        # ä¿å­˜ä¸º NetCDF (æ›´é€‚åˆç§‘å­¦æ•°æ®)
        netcdf_filepath = self.output_path / f"{sitename}_uWUE_output.nc"
        results.to_netcdf(netcdf_filepath)
        
        self.logger.info(f"  ğŸ’¾ ç»“æœå·²ä¿å­˜: {output_filename}")
    
    def _create_visualization(self, results, sitename):
        """åˆ›å»ºå¯è§†åŒ–å›¾è¡¨"""
        try:
            # åˆ›å»ºå›¾è¡¨ç›®å½•
            plots_dir = self.output_path / 'plots'
            plots_dir.mkdir(exist_ok=True)
            
            # è½¬æ¢ä¸º DataFrame ä»¥ä¾¿ç»˜å›¾
            df = results.to_dataframe().reset_index()
            df = df.dropna()  # ç§»é™¤ NaN å€¼
            
            if len(df) == 0:
                self.logger.warning(f"  âš ï¸ ç«™ç‚¹ {sitename} æ— æœ‰æ•ˆæ•°æ®ï¼Œè·³è¿‡ç»˜å›¾")
                return
            
            # åˆ›å»ºå¤šå­å›¾
            fig, axes = plt.subplots(2, 2, figsize=(15, 10))
            fig.suptitle(f'uWUE åˆ†æç»“æœ - {sitename}', fontsize=16, fontweight='bold')
            
            # 1. è’¸æ•£å‘æ—¶é—´åºåˆ—
            ax1 = axes[0, 0]
            ax1.plot(df['time'], df['ET'], 'b-', alpha=0.7, label='æ€»è’¸æ•£å‘ (ET)')
            ax1.plot(df['time'], df['zhou_T'], 'r-', alpha=0.8, label='è’¸è…¾ (T)')
            ax1.set_title('è’¸æ•£å‘ä¸è’¸è…¾æ—¶é—´åºåˆ—')
            ax1.set_ylabel('è’¸æ•£å‘ (mm dâ»Â¹)')
            ax1.legend()
            ax1.grid(True, alpha=0.3)
            
            # 2. è’¸è…¾æ¯”ä¾‹
            ax2 = axes[0, 1]
            df['T_ET_ratio'] = df['zhou_T'] / df['ET']
            ax2.plot(df['time'], df['T_ET_ratio'], 'g-', alpha=0.7)
            ax2.set_title('è’¸è…¾æ¯”ä¾‹ (T/ET)')
            ax2.set_ylabel('è’¸è…¾æ¯”ä¾‹')
            ax2.set_ylim(0, 1)
            ax2.grid(True, alpha=0.3)
            
            # 3. æ•£ç‚¹å›¾: ET vs T
            ax3 = axes[1, 0]
            scatter = ax3.scatter(df['ET'], df['zhou_T'], c=df.index, cmap='viridis', alpha=0.6)
            ax3.plot([0, df['ET'].max()], [0, df['ET'].max()], 'k--', alpha=0.5, label='1:1 çº¿')
            ax3.set_xlabel('æ€»è’¸æ•£å‘ (mm dâ»Â¹)')
            ax3.set_ylabel('è’¸è…¾ (mm dâ»Â¹)')
            ax3.set_title('è’¸æ•£å‘ vs è’¸è…¾')
            ax3.legend()
            ax3.grid(True, alpha=0.3)
            
            # 4. æœˆå¹³å‡å€¼
            ax4 = axes[1, 1]
            df['month'] = pd.to_datetime(df['time']).dt.month
            monthly_mean = df.groupby('month')[['ET', 'zhou_T']].mean()
            
            months = monthly_mean.index
            ax4.plot(months, monthly_mean['ET'], 'b-o', label='ET')
            ax4.plot(months, monthly_mean['zhou_T'], 'r-o', label='è’¸è…¾')
            ax4.set_title('æœˆå¹³å‡è’¸æ•£å‘')
            ax4.set_xlabel('æœˆä»½')
            ax4.set_ylabel('è’¸æ•£å‘ (mm dâ»Â¹)')
            ax4.set_xticks(range(1, 13))
            ax4.legend()
            ax4.grid(True, alpha=0.3)
            
            # æ ¼å¼åŒ–æ—¶é—´è½´
            for ax in [ax1, ax2]:
                ax.xaxis.set_major_formatter(DateFormatter('%Y'))
                ax.xaxis.set_major_locator(mdates.YearLocator())
            
            plt.tight_layout()
            
            # ä¿å­˜å›¾è¡¨
            plot_filename = plots_dir / f"{sitename}_uWUE_analysis.png"
            plt.savefig(plot_filename, dpi=300, bbox_inches='tight')
            plt.close()
            
            self.logger.info(f"  ğŸ“ˆ å¯è§†åŒ–å›¾è¡¨å·²ä¿å­˜: {plot_filename.name}")
            
        except Exception as e:
            self.logger.error(f"  âŒ åˆ›å»ºå¯è§†åŒ–å›¾è¡¨å¤±è´¥: {e}")
    
    def generate_summary_report(self):
        """ç”Ÿæˆå¤„ç†æ€»ç»“æŠ¥å‘Š"""
        self.logger.info("ğŸ“‹ ç”Ÿæˆå¤„ç†æ€»ç»“æŠ¥å‘Š...")
        
        # è®¡ç®—ç»Ÿè®¡ä¿¡æ¯
        total_time = sum(self.processing_stats['processing_times'])
        avg_time = np.mean(self.processing_stats['processing_times']) if self.processing_stats['processing_times'] else 0
        
        # åˆ›å»ºæ€»ç»“æŠ¥å‘Š
        report = f"""
â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘                                å¤„ç†æ€»ç»“æŠ¥å‘Š                                        â•‘
â• â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•£
â•‘ å¤„ç†æ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}                            â•‘
â•‘ æ€»æ–‡ä»¶å¤¹æ•°: {self.processing_stats['total_folders']}                               â•‘
â•‘ æˆåŠŸå¤„ç†: {self.processing_stats['processed_successfully']}                        â•‘
â•‘ å¤„ç†å¤±è´¥: {self.processing_stats['failed_processing']}                             â•‘
â•‘ æˆåŠŸç‡: {self.processing_stats['processed_successfully']/max(self.processing_stats['total_folders'],1)*100:.1f}%  â•‘
â•‘ æ€»è€—æ—¶: {total_time:.2f} ç§’                                                        â•‘
â•‘ å¹³å‡è€—æ—¶: {avg_time:.2f} ç§’/ç«™ç‚¹                                                   â•‘
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

æˆåŠŸå¤„ç†çš„ç«™ç‚¹:
{chr(10).join([f"  â€¢ {site}" for site in self.processing_stats['sites_processed']])}
        """
        
        print(report)
        
        # ä¿å­˜æŠ¥å‘Šåˆ°æ–‡ä»¶
        report_file = self.output_path / 'processing_summary.txt'
        with open(report_file, 'w', encoding='utf-8') as f:
            f.write(report)
        
        self.logger.info(f"ğŸ“„ æ€»ç»“æŠ¥å‘Šå·²ä¿å­˜: {report_file}")
    
    def run(self):
        """è¿è¡Œæ‰¹å¤„ç†ç¨‹åº"""
        self.print_header()
        
        # æ‰«æç›®å½•
        valid_folders = self.scan_directories()
        
        if not valid_folders:
            self.logger.warning("æœªæ‰¾åˆ°ç¬¦åˆæ¡ä»¶çš„æ–‡ä»¶å¤¹ï¼Œç¨‹åºç»“æŸ")
            return
        
        # å¤„ç†æ¯ä¸ªç«™ç‚¹
        for i, folder_path in enumerate(valid_folders, 1):
            self.logger.info(f"\n{'='*60}")
            self.logger.info(f"å¤„ç†è¿›åº¦: {i}/{len(valid_folders)} ({i/len(valid_folders)*100:.1f}%)")
            self.logger.info(f"å½“å‰æ–‡ä»¶å¤¹: {folder_path.name}")
            
            success, sitename, proc_time, results = self.process_single_site(folder_path)
            
            if success:
                self.logger.info(f"âœ… æˆåŠŸå¤„ç†ç«™ç‚¹ {sitename}")
            else:
                self.logger.error(f"âŒ å¤„ç†å¤±è´¥: {folder_path.name}")
        
        # ç”Ÿæˆæ€»ç»“æŠ¥å‘Š
        self.generate_summary_report()
        
        self.logger.info("\nğŸ‰ æ‰€æœ‰å¤„ç†å®Œæˆ!")


def main():
    """ä¸»å‡½æ•°"""
    # é…ç½®å‚æ•°
    # BASE_PATH = 'Z:\\LCM\\ET_T_Partition\\Test_Site'  # æµ‹è¯•æ•°æ®æºæ ¹ç›®å½•
    BASE_PATH = 'Z:\\Observation\\FLUXNET.4.0\\FLUXNET2015-Tier2'
    # BASE_PATH = 'Z:\\Observation\\AmeriFLUX'  # AmeriFLUXæ ¹ç›®å½•

    OUTPUT_PATH = 'Z:\\LCM\\ET_T_Partition\\uWUE\\uWUE_FLUXNET_Output'
    # OUTPUT_PATH = 'Z:\\LCM\\ET_T_Partition\\Test_Site'  # æµ‹è¯•ç›®å½•
    # OUTPUT_PATH = 'Z:\\LCM\\ET_T_Partition\\uWUE\\uWUE_AmeriFLUX_Output'  # AmeriFLUXè¾“å‡ºç›®å½•
    CREATE_PLOTS = True  # æ˜¯å¦ç”Ÿæˆå¯è§†åŒ–å›¾è¡¨
    
    # åˆ›å»ºå¤„ç†å™¨å¹¶è¿è¡Œ
    processor = uWUEBatchProcessor(BASE_PATH, OUTPUT_PATH, CREATE_PLOTS)
    processor.run()


if __name__ == "__main__":
    main()